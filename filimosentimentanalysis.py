# -*- coding: utf-8 -*-
"""filimosentimentanalysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LkybCzqevL7nC-pqpxTvDf5nXwCrMtna

# Import required packages
"""

! pip install hazm
import hazm
! pip install autocorrect
from autocorrect import Speller 
! pip install clean-text
from cleantext import clean
! pip install word2vec
import word2vec

import pandas as pd
import numpy as np
import nltk
from string import punctuation
import re
import nltk 
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Dense,Embedding,LSTM,SpatialDropout1D
from tensorflow.keras.models import Sequential
from sklearn.model_selection import train_test_split
from sklearn.svm import LinearSVC ,SVC
from sklearn.ensemble import RandomForestClassifier 
from sklearn.naive_bayes import GaussianNB

"""# Data reading From Google drive"""

from google.colab import drive

drive.mount('/content/drive', force_remount=True)

!ls '/content/drive/MyDrive/data.csv'

data = pd.read_csv('/content/drive/MyDrive/data.csv', encoding='utf-8')
df = pd.DataFrame(data)
df.drop(columns=['Unnamed: 0', 'Unnamed: 0.1'], inplace=True)

data = df
data

"""# Data Cleaning"""

# print data information
print('data information')
print(data.info(), '\n')

# print missing values information
print('missing values stats')
print(data.isnull().sum(), '\n')

# handle some conflicts with the dataset structure
# you can find a reliable solution, for the sake of the simplicity
# I just remove these bad combinations!


data = data.dropna(subset=['film_name'])
data = data.dropna(subset=['comment_text'])

data = data.reset_index(drop=True)




# print data information
print('data information')
print(data.info(), '\n')

# print missing values information
print('missing values stats')
print(data.isnull().sum(), '\n')

# print some missing values
print('some missing values')
print(data[data['film_name'].isnull()].iloc[:5], '\n')

data

"""# Texet Cleaning (Text Normalize) """

def sentence_tokenize(text):
    return nltk.sent_tokenize(text)

def word_tokenize(text):
       
        return nltk.word_tokenize(text)

import nltk
nltk.download('punkt')

data['comment_text'].apply(sentence_tokenize)

def remove_numbers(text):
        """
        take string input and return a clean text without numbers.
        Use regex to discard the numbers.
        """
        output = ''.join(c for c in text if not c.isdigit())
        return output

def remove_punct(text):
  
        return ''.join(c for c in text if c not in punctuation)

def remove_wierds(text):
  wierd_pattern = re.compile("["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
        u"\U00002702-\U000027B0"
        u"\U000024C2-\U0001F251"
        u"\U0001f926-\U0001f937"
        u'\U00010000-\U0010ffff'
        u"\u200d"
        u"\u2640-\u2642"
        u"\u2600-\u2B55"
        u"\u23cf"
        u"\u23e9"
        u"\u231a"
        u"\u3030"
        u"\ufe0f"
        u"\u2069"
        u"\u2066"
        u"\u200c"
        u"\u2068"
        u"\u2067"
        "]+", flags=re.UNICODE)
  text = wierd_pattern.sub(r'', text)
  return text

clean_data = data['comment_text'].apply(remove_wierds)

data['clean_comment'] = clean_data

data.head()

data.drop(columns=['comment_text'], inplace=True)

data.head(5)

"""# Building My Model

### build phreser for vocabulary
"""

from gensim.models.phrases import Phrases, Phraser

sent = [row.split() for row in data['clean_comment']]
phrases = Phrases(sent, min_count=30)
bigram = Phraser(phrases)

sentences = bigram[sent]

"""### word frequncy"""

from collections import defaultdict 
word_freq = defaultdict(int)
for sent in sentences:
    for i in sent:
        word_freq[i] += 1
len(word_freq)

sorted_word_freq = sorted(word_freq, key=word_freq.get, reverse=True)[:20]

"""### Training Model"""

import multiprocessing
from gensim.models import Word2Vec

cores = multiprocessing.cpu_count()

w2v_model = Word2Vec(min_count=2,
                     window=2,
                     size=100,
                     sample=6e-5, 
                     alpha=0.02, 
                     min_alpha=0.0007, 
                     negative=20,
                     workers=cores-1)

from time import time
t = time()

w2v_model.build_vocab(sentences, progress_per=10000)

print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 5)))

t = time()

w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)

print('Time to train the model: {} mins'.format(round((time() - t) / 60, 5)))

w2v_model.init_sims(replace=True)

"""## Example Getting word similarity for weight words"""

w2v_model.wv.most_similar(negative=["عالی"])

"""## Set vector and PCA for must freq words"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
 
import seaborn as sns
sns.set_style("darkgrid")

from sklearn.decomposition import PCA
from sklearn.manifold import TSNE

def tsnescatterplot(model, word, list_names):
    arrays = np.empty((0, 100), dtype='f')
    word_labels = [word]
    color_list  = ['red']

    # adds the vector of the query word
    arrays = np.append(arrays, model.wv.__getitem__([word]), axis=0)
    
    # gets list of most similar words
    close_words = model.wv.most_similar([word])
    
    # adds the vector for each of the closest words to the array
    for wrd_score in close_words:
        wrd_vector = model.wv.__getitem__([wrd_score[0]])
        word_labels.append(wrd_score[0])
        color_list.append('blue')
        arrays = np.append(arrays, wrd_vector, axis=0)
    
    # adds the vector for each of the words from list_names to the array
    for wrd in list_names:
        wrd_vector = model.wv.__getitem__([wrd])
        word_labels.append(wrd)
        color_list.append('green')
        arrays = np.append(arrays, wrd_vector, axis=0)
        
    # Reduces the dimensionality from 100 to 20 dimensions with PCA
    reduc = PCA(n_components=20).fit_transform(arrays)
    
    # Finds t-SNE coordinates for 2 dimensions
    np.set_printoptions(suppress=True)
    
    Y = TSNE(n_components=2, random_state=0, perplexity=15).fit_transform(reduc)
    
    # Sets everything up to plot
    df = pd.DataFrame({'x': [x for x in Y[:, 0]],
                       'y': [y for y in Y[:, 1]],
                       'words': word_labels,
                       'color': color_list})
    
    fig, _ = plt.subplots()
    fig.set_size_inches(9, 9)
    
    # Basic plot
    p1 = sns.regplot(data=df,
                     x="x",
                     y="y",
                     fit_reg=False,
                     marker="o",
                     scatter_kws={'s': 40,
                                  'facecolors': df['color']
                                 }
                    )
    
    # Adds annotations one by one with a loop
    for line in range(0, df.shape[0]):
         p1.text(df["x"][line],
                 df['y'][line],
                 '  ' + df["words"][line].title(),
                 horizontalalignment='left',
                 verticalalignment='bottom', size='medium',
                 color=df['color'][line],
                 weight='normal'
                ).set_size(15)

    
    plt.xlim(Y[:, 0].min()-50, Y[:, 0].max()+50)
    plt.ylim(Y[:, 1].min()-50, Y[:, 1].max()+50)
            
    plt.title('t-SNE visualization for {}'.format(word.title()))

listofword = ['عالی', 'قشنگ', 'پیشنهاد', 'زیبا', 'بود', 'فوق','العاده', 'واقعا', 'جالب']
tsnescatterplot(w2v_model, 'خوب', listofword)

tsnescatterplot(w2v_model, 'عالی', [i[0] for i in w2v_model.wv.most_similar(negative=["عالی"])])

"""##Getting word similarity for weight words"""

x = "عالی"
w2v_model.wv.most_similar(positive=[x])

x = "خوب"
w2v_model.wv.most_similar(positive=[x])

x = "قشنگ"
w2v_model.wv.most_similar(positive=[x])

x = "پیشنهاد"
w2v_model.wv.most_similar(positive=[x])

x = "عالی"
y = "پیشنهاد"
z = "قشنگ"
v = "خوب"
word_predict_instance = w2v_model.wv.most_similar(positive=[x, y, z, v])

word_predict_instance

"""## sentiment prediction"""

type(word_predict_instance)

nude_word = [x[0] for x in word_predict_instance]

nude_word

comment_box = []
for comment in data['clean_comment']:
  comment_box.append(comment)

comment_box

from hazm import word_tokenize


word_token = word_tokenize(str(comment_box))

! pip install datasketch

"""## Jacard similarity for get similarity point of must freq word with comments """

from datasketch import MinHash

data1 = ['بازی', 'عالی','پیشنهاد','خوب', 'بود', 'عالیه','بد', 'نبود', 'فیلم']
data2 = word_token

m1, m2 = MinHash(), MinHash()
for d in data1:
    m1.update(d.encode('utf8'))
for d in data2:
    m2.update(d.encode('utf8'))
print("Estimated Jaccard for data1 and data2 is", m1.jaccard(m2))
s1 = set(data1)
s2 = set(data2)
actual_jaccard = float(len(s1.intersection(s2)))/float(len(s1.union(s2)))
print("Actual Jaccard for data1 and data2 is", actual_jaccard)

"""## Cosine Similarity for get similarity point of must freq word with comments"""

from collections import Counter
from math import sqrt

def word2vec(word):
    # count the characters in word
    cw = Counter(word)
    # precomputes a set of the different characters
    sw = set(cw)
    # precomputes the "length" of the word vector
    lw = sqrt(sum(c*c for c in cw.values()))
    return cw, sw, lw

def cosdis(v1, v2):
    # which characters are common to the two words?
    common = v1[1].intersection(v2[1])
    # by definition of cosine distance we have
    return sum(v1[0][ch]*v2[0][ch] for ch in common)/v1[2]/v2[2]


list_A = ['عالی']
list_B = comment_box

result1 = []

for key in list_A:
    for word in list_B:
      res = cosdis(word2vec(word), word2vec(key))
      result1.append(res)
      print("The cosine similarity between : {} and : {} is: {}".format(word, key, res*100))

result1

"""# Now We See Cosine Similarity Have Better Result

## Work Cosine Similarity For other weited words
"""

d_f = pd.DataFrame()

d_f['perfect_rate'] = result1

def word2vec(word):
    # count the characters in word
    cw = Counter(word)
    # precomputes a set of the different characters
    sw = set(cw)
    # precomputes the "length" of the word vector
    lw = sqrt(sum(c*c for c in cw.values()))
    return cw, sw, lw

def cosdis(v1, v2):
    # which characters are common to the two words?
    common = v1[1].intersection(v2[1])
    # by definition of cosine distance we have
    return sum(v1[0][ch]*v2[0][ch] for ch in common)/v1[2]/v2[2]


list_A = ['خوب']
list_B = comment_box

result2 = []

for key in list_A:
    for word in list_B:
      res = cosdis(word2vec(word), word2vec(key))
      result2.append(res)
      #print("The cosine similarity between : {} and : {} is: {}".format(word, key, res*100))

d_f['good_rate'] = result2

def word2vec(word):
    # count the characters in word
    cw = Counter(word)
    # precomputes a set of the different characters
    sw = set(cw)
    # precomputes the "length" of the word vector
    lw = sqrt(sum(c*c for c in cw.values()))
    return cw, sw, lw

def cosdis(v1, v2):
    # which characters are common to the two words?
    common = v1[1].intersection(v2[1])
    # by definition of cosine distance we have
    return sum(v1[0][ch]*v2[0][ch] for ch in common)/v1[2]/v2[2]


list_A = ['پیشنهاد']
list_B = comment_box

result4 = []

for key in list_A:
    for word in list_B:
      res = cosdis(word2vec(word), word2vec(key))
      result4.append(res)
      #print("The cosine similarity between : {} and : {} is: {}".format(word, key, res*100))

d_f['propose_rate'] = result4

def word2vec(word):
    # count the characters in word
    cw = Counter(word)
    # precomputes a set of the different characters
    sw = set(cw)
    # precomputes the "length" of the word vector
    lw = sqrt(sum(c*c for c in cw.values()))
    return cw, sw, lw

def cosdis(v1, v2):
    # which characters are common to the two words?
    common = v1[1].intersection(v2[1])
    # by definition of cosine distance we have
    return sum(v1[0][ch]*v2[0][ch] for ch in common)/v1[2]/v2[2]


list_A = ['بد']
list_B = comment_box

result3 = []

for key in list_A:
    for word in list_B:
      res = cosdis(word2vec(word), word2vec(key))
      result3.append(res)
      #print("The cosine similarity between : {} and : {} is: {}".format(word, key, res*100))

d_f['bad_rate'] = result3

def word2vec(word):
    # count the characters in word
    cw = Counter(word)
    # precomputes a set of the different characters
    sw = set(cw)
    # precomputes the "length" of the word vector
    lw = sqrt(sum(c*c for c in cw.values()))
    return cw, sw, lw

def cosdis(v1, v2):
    # which characters are common to the two words?
    common = v1[1].intersection(v2[1])
    # by definition of cosine distance we have
    return sum(v1[0][ch]*v2[0][ch] for ch in common)/v1[2]/v2[2]


list_A = ['نبود']
list_B = comment_box

result5 = []

for key in list_A:
    for word in list_B:
      res = cosdis(word2vec(word), word2vec(key))
      result5.append(res)
      #print("The cosine similarity between : {} and : {} is: {}".format(word, key, res*100))

d_f['not_rate'] = result5

def word2vec(word):
    # count the characters in word
    cw = Counter(word)
    # precomputes a set of the different characters
    sw = set(cw)
    # precomputes the "length" of the word vector
    lw = sqrt(sum(c*c for c in cw.values()))
    return cw, sw, lw

def cosdis(v1, v2):
    # which characters are common to the two words?
    common = v1[1].intersection(v2[1])
    # by definition of cosine distance we have
    return sum(v1[0][ch]*v2[0][ch] for ch in common)/v1[2]/v2[2]


list_A = ['فیلم']
list_B = comment_box

result6 = []

for key in list_A:
    for word in list_B:
      res = cosdis(word2vec(word), word2vec(key))
      result6.append(res)
     # print("The cosine similarity between : {} and : {} is: {}".format(word, key, res*100))

d_f['film_rate'] = result6

def word2vec(word):
    # count the characters in word
    cw = Counter(word)
    # precomputes a set of the different characters
    sw = set(cw)
    # precomputes the "length" of the word vector
    lw = sqrt(sum(c*c for c in cw.values()))
    return cw, sw, lw

def cosdis(v1, v2):
    # which characters are common to the two words?
    common = v1[1].intersection(v2[1])
    # by definition of cosine distance we have
    return sum(v1[0][ch]*v2[0][ch] for ch in common)/v1[2]/v2[2]


list_A = ['بازی']
list_B = comment_box

result7 = []

for key in list_A:
    for word in list_B:
      res = cosdis(word2vec(word), word2vec(key))
      result7.append(res)
      #print("The cosine similarity between : {} and : {} is: {}".format(word, key, res*100))

d_f['roleplay_rate'] = result7

"""### I Get point of similarity for weited words and build dataframe with that"""

d_f

d_f['film_name'] = data['film_name']

d_f

d_f['comment'] = data['clean_comment']

df = d_f

df.head()

df = df[['comment', 'film_name', 'roleplay_rate', 'film_rate', 'not_rate', 'bad_rate', 'propose_rate', 'good_rate', 'perfect_rate']]

"""### Now We have New DataFrame with comments, film_name label and points of similariy must weighted words for each comments"""

df.head(10)

"""## adding new columns Nmae that sentiment for set sentiment for each coment"""

df['sentiment'] = 1.0

df.head()

"""## Now for Set Sentiment labels Calcuting Mean Of rate point """

def rate_mean():
  x = (df['roleplay_rate'] + df['film_rate'] + df['not_rate'] + 
       df['bad_rate'] + df['propose_rate'] + df['good_rate'] +
       df['perfect_rate'] )
  rs = x / 7
  return rs

df['rate_mean'] = rate_mean()

df.head()

"""## Set Label"""

A = []
for i in df['rate_mean']:
  if i > 0.30:
    A.append('positive')
  else:
    A.append("negative")  
  
df['sentiment'] = A

df

df['sentiment'].unique()

import matplotlib.pyplot as plt

plt.hist(df['sentiment'])
plt.show()

"""#Conclusion"""

df.drop(columns=['roleplay_rate','comment','perfect_rate', 'good_rate', 'film_rate', 'not_rate', 'bad_rate', 'propose_rate', 'rate_mean'], axis=1, inplace=True)

df.head(10)

df.groupby(by=["sentiment"]).count()

df.groupby(by=["film_name"]).count()

print(df.groupby(by=["sentiment"]).count())
print(f'percent of data label for each film: {10 / 128 *100}')

def percent_of_negative_lebel_for_each_film(x):
  perc = 10 /128 
  neg_perc = perc * x
  return neg_perc

print(f' Score of Ngative Sentiment for "BalckCats" film: {format(percent_of_negative_lebel_for_each_film(18), ".2f")}')
print(f' Score of Ngative Sentiment for "Death_of_Salesman" film: {format(percent_of_negative_lebel_for_each_film(10), ".2f")}')
print(f' Score of Ngative Sentiment for "Departed" film: {format(percent_of_negative_lebel_for_each_film(15), ".2f")}')
print(f' Score of Ngative Sentiment for "Facing_Mirrors" film: {format(percent_of_negative_lebel_for_each_film(23), ".2f")}')
print(f' Score of Ngative Sentiment for "LoserMan" film: {format(percent_of_negative_lebel_for_each_film(22), ".2f")}')
print(f' Score of Ngative Sentiment for "NoChoice" film: {format(percent_of_negative_lebel_for_each_film(20), ".2f")}')
print(f' Score of Ngative Sentiment for "The_Late_Father" film: {format(percent_of_negative_lebel_for_each_film(20), ".2f")}')

def percent_of_psitive_lebel_for_each_film(x):
  perc = 118 /128 
  pos_perc = perc * x
  return pos_perc

print(f' Score of Positive Sentiment for "BalckCats" film: {format(percent_of_psitive_lebel_for_each_film(18), ".2f")}')
print(f' Score of Positive Sentiment for "Death_of_Salesman	" film: {format(percent_of_psitive_lebel_for_each_film(10), ".2f")}')
print(f' Score of Positive Sentiment for "Departed" film: {format(percent_of_psitive_lebel_for_each_film(15), ".2f")}')
print(f' Score of Positive Sentiment for "Facing_Mirrors" film: {format(percent_of_psitive_lebel_for_each_film(23), ".2f")}')
print(f' Score of Positive Sentiment for "LoserMan" film: {format(percent_of_psitive_lebel_for_each_film(22), ".2f")}')
print(f' Score of Positive Sentiment for "NoChoice" film: {format(percent_of_psitive_lebel_for_each_film(20), ".2f")}')
print(f' Score of Positive Sentiment for "The_Late_Father" film: {format(percent_of_psitive_lebel_for_each_film(20), ".2f")}')